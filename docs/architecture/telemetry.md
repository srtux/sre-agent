# Telemetry and Observability in SRE Agent

This document describes the telemetry and observability architecture for the SRE Agent.

## Hybrid Telemetry Strategy (February 2026)

As of February 2026, the SRE Agent uses a **Hybrid Telemetry Strategy**. While we still rely on the [Google Agent Development Kit (ADK)](https://github.com/google/adk) for core orchestration, we have introduced high-fidelity native instrumentation for the underlying Gemini models.

### Core Principles

1.  **Native GenAI Instrumentation**: We use the `GoogleGenAiSdkInstrumentor` to capture the "black box" reasoning inside Gemini calls. This captures detailed prompt/response pairs with native OTel semantics.
2.  **Multi-Receiver Coexistence Pattern**: Supports simultaneous export to both **Langfuse** (for agentic debugging) and **Google Cloud Trace** (for production observability).
3.  **Automatic High-Fidelity Capture**: Enabled by setting `OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT=true`.
4.  **Zero-Invasive Tools**: Individual tool implementations remain clean and rely on the `@adk_tool` decorator for automatic span generation.

---

## Configuration

Telemetry is now primarily configured via environment variables that affect the Underlying ADK and runtime.

### General Settings

*   `LOG_LEVEL`: One of `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`. (Default: `INFO`).
*   `LOG_FORMAT`: `TEXT` (default) for readable console logs, or `JSON` for structured logs in production.
*   `DISABLE_TELEMETRY`: Set to `true` during tests to prevent unnecessary export overhead.

### Log Noise Reduction

To keep logs readable, the following "chatty" libraries are explicitly silenced to `WARNING` level, ignoring the global `LOG_LEVEL`:
*   `google.auth`, `urllib3`, `grpc`, `httpx`, `httpcore`, `aiosqlite`
*   `google_adk.google.adk.models.google_llm`

**Configuration**: These overrides are defined in `sre_agent/tools/common/telemetry.py` inside `setup_telemetry()`. To see debug logs from these libraries, you must modify the list in the source code.

### ADK / Agent Engine native Tracing

To enable full visibility into agent reasoning:

*   `OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT=true`: Enables high-fidelity prompt/response capture in Cloud Trace and Langfuse.
*   `OTEL_TO_CLOUD=true`: Enables the Cloud Trace exporter for native GCP observability.
*   `SUPPRESS_OTEL_ERRORS=true`: Suppresses non-critical OTLP export warnings to keep logs clean.

---

## Technical Architecture

### Tool Visibility

Every tool marked with `@adk_tool` automatically benefits from:
1.  **Standard Logging**: Input arguments and output status are logged to `stdout`.
2.  **Native Span Generation**: When running in Agent Engine, tool calls are automatically converted into spans by the platform.

### Standardized Tool Responses

All tools must return a `BaseToolResponse` (serialized as JSON string). This ensures that even without manual OTel code, the agent's performance and error rates can be monitored via the logical status of these objects.

```python
from sre_agent.schema import BaseToolResponse, ToolStatus

@adk_tool
async def my_tool(...) -> str:
    # Logic here
    return json.dumps(BaseToolResponse(
        status=ToolStatus.SUCCESS,
        result={"data": ...}
    ).model_dump())
```

### Callback Pipeline Integration

The agent's callback pipeline provides additional telemetry beyond basic tool spans:

1. **`before_model_callback` / `after_model_callback`** (`core/model_callbacks.py`): Track token usage, cost per LLM call, and enforce token budgets.
2. **`composite_after_tool_callback`**: Chains large payload handling, truncation, and memory recording.
3. **`after_agent_memory_callback`** (`memory/callbacks.py`): Records agent turn outcomes for continuous learning.

These callbacks integrate with the OTel spans automatically generated by `@adk_tool` and ADK's native instrumentation.

### Removal of Legacy Components

The following legacy components have been **removed**:
*   `ArizeAxExporter` and all `arize` package dependencies.
*   `setup_telemetry()` logic for manual `MetricReader` and `TraceProvider` registration.
*   Manual `opentelemetry-api` and `opentelemetry-sdk` direct imports in tools.

---

## Langfuse Tracing (Local & Debugging)

While we prefer native ADK tracing for production, **Langfuse** is supported for local development to visualize complex agent reasoning chains and tool use.

### Enabling Langfuse

1.  **Dependencies**: Ensure dev dependencies are installed (`uv sync`).
2.  **Run Langfuse locally**: `docker compose up` (see [Langfuse self-hosting docs](https://langfuse.com/docs/deployment/self-host)).
3.  **Environment Variables**:
    *   `LANGFUSE_TRACING=true`
    *   `LANGFUSE_PUBLIC_KEY=<your-public-key>`
    *   `LANGFUSE_SECRET_KEY=<your-secret-key>`
    *   `LANGFUSE_HOST=http://localhost:3000` (default for local self-hosted)

### Features

*   **Full Trace Trees**: Visualizes the complete execution path from user input -> agent reasoning -> tool calls -> final response.
*   **Session View**: Groups interactions by session ID, allowing you to see the full conversation history.
*   **Metadata**: Automatically captures user ID and session tags if available.
*   **Scores & Evaluations**: Built-in support for LLM-as-Judge and custom evaluators.

**Note**: In the new hybrid strategy, Langfuse tracing can coexist with Cloud Trace if `OTEL_TO_CLOUD=true` and `LANGFUSE_TRACING=true` are both set.

---

## Observability Best Practices

1.  **Use Logging for Context**: Instead of a manual span attribute, use `logger.info()` or `logger.debug()`. These are automatically correlated with the active trace by the Agent Engine logging service.
2.  **Trust the Decorator**: Let `@adk_tool` handle the standard execution tracing.
3.  **Monitor via BigQuery**: For historical analysis, use BigQuery tools to query the `_AllSpans` table (where ADK exports spans) rather than searching for manual instrumentation code.

---

## Agent Performance Monitoring (AgentOps)

The telemetry captured by the ADK and OTel instrumentation described above feeds directly into the AgentOps observability surfaces:

- **Agent Graph**: Aggregates OTel spans from BigQuery into an interactive topology showing agent-tool delegation, error rates, and cost -- see [Agent Graph](../concepts/agent_graph.md)
- **AgentOps Dashboard**: Provides fleet-wide KPIs, latency/QPS charts, and model/tool performance tables -- see [AgentOps Dashboard Guide](../guides/agent_ops_dashboard.md)

For the conceptual overview, see [Multi-Agent Observability](../concepts/multi_agent_observability.md).

---
*Last verified: 2026-02-21 -- Auto SRE Team*
